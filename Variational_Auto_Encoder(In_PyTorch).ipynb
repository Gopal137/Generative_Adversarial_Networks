{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational_Auto_Encoder(In PyTorch).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzc9HtLkyb2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Loading all the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6WHzowVzLf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nodMeWOazPbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(\n",
        "        './data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms\n",
        "        )\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "        './data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transforms\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svlFMYxfzZB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100     \n",
        "N_EPOCHS = 50      \n",
        "INPUT_DIM = 28 * 28 \n",
        "HIDDEN_DIM = 512\n",
        "#Introduction of another linear layer to increase performance\n",
        "HIDDEN_DIM_2=256 \n",
        "LATENT_DIM = 2\n",
        "lr = 1e-3           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IGSDQLQzt45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Instantiate the  iterator objects for the train ans the test dataset\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4I3_MDz6SW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn1moiKn0CUD",
        "colab_type": "text"
      },
      "source": [
        "**CREATING THE ENCODER ARCHITECTURE**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbWuKwS50Ohe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_dim,hidden_dim,hidden_dim_2,z_dim):#Z_dim is the dimensionality of the latent space\n",
        "    super().__init__()\n",
        "    self.linear=nn.Linear(input_dim,hidden_dim)\n",
        "    self.linear_2=nn.Linear(hidden_dim,hidden_dim_2)#Second hidden layer \n",
        "    self.mean=nn.Linear(hidden_dim_2,z_dim)#This layer trains the mean_parameter of the latent space  distribution\n",
        "    self.variance=nn.Linear(hidden_dim_2,z_dim)#This layer trains the variance parameter of the distribution\n",
        "  def forward(self,x):\n",
        "    #x is of the shape [batch_size,input_dim]\n",
        "    hidden=F.relu(self.linear(x))\n",
        "    hidden_2=F.relu(self.linear_2(hidden))\n",
        "    z_mean=self.mean(hidden_2)#z_mean and z_variance have the shape [hidden_dim,latent_space_dim]\n",
        "    z_variance=self.variance(hidden_2)\n",
        "    return z_mean,z_variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J1H5ggf2icZ",
        "colab_type": "text"
      },
      "source": [
        "**CREATING THE DECODER ARHITECTURE**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF42Gd6o2oJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,z_dim,hidden_dim_2,hidden_dim,output_dim):\n",
        "    ##The basic aim here is take input from the latent dimension create an intermaediate trainable layer and output it to the original dimension of the figure i.e 28*28\n",
        "    super().__init__()\n",
        "    self.linear=nn.Linear(z_dim,hidden_dim_2)\n",
        "    self.linear_2=nn.Linear(hidden_dim_2,hidden_dim)\n",
        "    self.out=nn.Linear(hidden_dim,output_dim)\n",
        "  def forward(self,x):\n",
        "    hidden=F.relu(self.linear(x))\n",
        "    hidden_2=F.relu(self.linear_2(hidden))\n",
        "    predicted=torch.sigmoid(self.out(hidden_2))\n",
        "    return predicted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45GGp1CH4Zh8",
        "colab_type": "text"
      },
      "source": [
        "**COMBINING THE ENCODER AND THE DECODER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81GUjG5a4fuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAEncoder(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super().__init__()\n",
        "    self.enc=encoder\n",
        "    self.dec=decoder\n",
        "  def forward(self,x):\n",
        "    #The outputs from the encoder z_mean and z_variance\n",
        "    z_mean,z_variance=self.enc(x)\n",
        "    #Sample from the distribution having latent parameters z_mean and z_variance\n",
        "    std=torch.exp(z_mean/2)\n",
        "    eps=torch.randn_like(std)\n",
        "    x_sample=eps.mul(std).add(z_mean)\n",
        "    ##Get the preditions\n",
        "    predicted=self.dec(x_sample)\n",
        "    return predicted,z_mean,z_variance\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPY744p0_rzT",
        "colab_type": "text"
      },
      "source": [
        "**INSTANTIAtING THE NETWORK**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78-4clvQ_3LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # encoder\n",
        "encoder = Encoder(INPUT_DIM, HIDDEN_DIM,HIDDEN_DIM_2, LATENT_DIM)\n",
        "\n",
        "    # decoder\n",
        "decoder = Decoder(LATENT_DIM, HIDDEN_DIM_2,HIDDEN_DIM, INPUT_DIM)\n",
        "\n",
        "    # vae\n",
        "model = VAEncoder(encoder, decoder).to(device)\n",
        "\n",
        "    # optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhuCAbB-4Yrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "        # set the train mode\n",
        "        model.train()\n",
        "\n",
        "        # loss of the epoch\n",
        "        train_loss = 0\n",
        "\n",
        "        for i, (x, _) in enumerate(train_iterator):\n",
        "            # reshape the data into [batch_size, 784]\n",
        "            x = x.view(-1, 28 * 28)\n",
        "            x = x.to(device)\n",
        "            \n",
        "            # update the gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward pass\n",
        "            x_sample, z_mu, z_var = model(x)\n",
        "\n",
        "            # reconstruction loss\n",
        "            recon_loss = F.binary_cross_entropy(x_sample, x, reduction='sum')\n",
        "\n",
        "            # kl divergence loss\n",
        "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
        "\n",
        "            # total loss\n",
        "            loss = recon_loss + kl_loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            # update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "        return train_loss\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "        # set the evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # test loss for the data\n",
        "        test_loss = 0\n",
        "\n",
        "        # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
        "        with torch.no_grad():\n",
        "            for i, (x, _) in enumerate(test_iterator):\n",
        "                # reshape the data\n",
        "                x = x.view(-1, 28 * 28)\n",
        "                x = x.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                x_sample, z_mu, z_var = model(x)\n",
        "\n",
        "                # reconstruction loss\n",
        "                recon_loss = F.binary_cross_entropy(x_sample, x, reduction='sum')\n",
        "                \n",
        "                # kl divergence loss\n",
        "                kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
        "                \n",
        "                # total loss\n",
        "                loss = recon_loss + kl_loss\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        return test_loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHS6ThuV4Xeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "outputId": "2490db32-c1a9-4f08-e5ba-590b0f73c35f"
      },
      "source": [
        "best_test_loss = float('inf')\n",
        "\n",
        "for e in range(N_EPOCHS):\n",
        "\n",
        "        train_loss = train()\n",
        "        test_loss = test()\n",
        "\n",
        "        train_loss /= len(train_dataset)\n",
        "        test_loss /= len(test_dataset)\n",
        "\n",
        "        print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}')\n",
        "\n",
        "        if best_test_loss > test_loss:\n",
        "            best_test_loss = test_loss\n",
        "            patience_counter = 1\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter > 5:\n",
        "            break\n",
        "        for g in range(10):\n",
        "          z = torch.randn(1, LATENT_DIM).to(device)\n",
        "\n",
        "           # run only the decoder\n",
        "          reconstructed_img = model.dec(z)\n",
        "          img = reconstructed_img.view(28, 28).data\n",
        "\n",
        "\n",
        "          plt.imshow(img.cpu().numpy(), cmap='gray')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Train Loss: 173.67, Test Loss: 172.02\n",
            "Epoch 1, Train Loss: 170.26, Test Loss: 170.01\n",
            "Epoch 2, Train Loss: 168.08, Test Loss: 167.16\n",
            "Epoch 3, Train Loss: 166.37, Test Loss: 165.96\n",
            "Epoch 4, Train Loss: 165.34, Test Loss: 165.07\n",
            "Epoch 5, Train Loss: 164.58, Test Loss: 164.20\n",
            "Epoch 6, Train Loss: 163.75, Test Loss: 163.81\n",
            "Epoch 7, Train Loss: 162.98, Test Loss: 163.02\n",
            "Epoch 8, Train Loss: 162.50, Test Loss: 162.09\n",
            "Epoch 9, Train Loss: 161.82, Test Loss: 162.34\n",
            "Epoch 10, Train Loss: 161.64, Test Loss: 162.27\n",
            "Epoch 11, Train Loss: 161.08, Test Loss: 161.36\n",
            "Epoch 12, Train Loss: 160.74, Test Loss: 161.29\n",
            "Epoch 13, Train Loss: 160.31, Test Loss: 160.74\n",
            "Epoch 14, Train Loss: 160.20, Test Loss: 160.36\n",
            "Epoch 15, Train Loss: 159.99, Test Loss: 160.17\n",
            "Epoch 16, Train Loss: 159.73, Test Loss: 160.29\n",
            "Epoch 17, Train Loss: 159.48, Test Loss: 160.22\n",
            "Epoch 18, Train Loss: 159.28, Test Loss: 159.70\n",
            "Epoch 19, Train Loss: 159.15, Test Loss: 159.27\n",
            "Epoch 20, Train Loss: 158.93, Test Loss: 159.02\n",
            "Epoch 21, Train Loss: 158.75, Test Loss: 159.48\n",
            "Epoch 22, Train Loss: 158.43, Test Loss: 158.96\n",
            "Epoch 23, Train Loss: 158.49, Test Loss: 158.79\n",
            "Epoch 24, Train Loss: 158.25, Test Loss: 158.94\n",
            "Epoch 25, Train Loss: 158.20, Test Loss: 158.65\n",
            "Epoch 26, Train Loss: 157.90, Test Loss: 158.38\n",
            "Epoch 27, Train Loss: 157.81, Test Loss: 158.45\n",
            "Epoch 28, Train Loss: 157.71, Test Loss: 158.24\n",
            "Epoch 29, Train Loss: 157.78, Test Loss: 158.63\n",
            "Epoch 30, Train Loss: 157.63, Test Loss: 159.11\n",
            "Epoch 31, Train Loss: 157.50, Test Loss: 157.81\n",
            "Epoch 32, Train Loss: 157.25, Test Loss: 157.94\n",
            "Epoch 33, Train Loss: 157.25, Test Loss: 158.10\n",
            "Epoch 34, Train Loss: 157.27, Test Loss: 158.24\n",
            "Epoch 35, Train Loss: 157.27, Test Loss: 157.92\n",
            "Epoch 36, Train Loss: 157.08, Test Loss: 158.06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngrADSRB2IAnCDiRB2IEk/gRebBgxGiMu+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PG4NI0nA_Sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "56678ef7-a5d7-4ff2-c24b-bfbe55ea256d"
      },
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        " # sample and generate a image\n",
        "with torch.no_grad(): \n",
        "      z = torch.randn(64, LATENT_DIM).to(device)\n",
        "\n",
        "      # run only the decoder\n",
        "      reconstructed_img = model.dec(z)\n",
        "      img = reconstructed_img.view(64,1,28, 28).data\n",
        "\n",
        "      print(z.shape)\n",
        "      print(img.shape)\n",
        "\n",
        "      \n",
        "      save_image(img, 'sample_' + '.png')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 2])\n",
            "torch.Size([64, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ab6h6FqNvjD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "d56856a9-3742-4792-b558-92ab1a1aa4be"
      },
      "source": [
        "with torch.no_grad():\n",
        "    z = torch.randn(64, 2).cuda()\n",
        "    sample = VAEncoder.decoder(z).cuda()\n",
        "    \n",
        "    save_image(sample.view(64, 1, 28, 28), 'sample_' + '.png')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-305e3f810f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'VAEncoder' has no attribute 'decoder'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfpHtid0A-I",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}